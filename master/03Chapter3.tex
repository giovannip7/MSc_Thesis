\section{Introduction}
\\
\\
\\
\begin{center}
    \textit{<<In the twenty-first century the \\
    robot will take the place which \\
    slave labor occupied in ancient \\ 
    civilization. There is no reason at \\ 
    all why most of this should not \\
    come to pass in less than a century, \\
    freeing mankind to pursue its \\
    higher aspirations.>>} \\ 
            \text{Nikola Tesla (1856 - 1943) }
\end{center}


\begin{center}
    \textit{<<Robots of the world! \\
    The power of
man has fallen!\\ A new world has
arisen:\\ the Rule of the Robots!
March!>>}\\
    \text{Rossum's Universal Robot (1920)}
\end{center}

Man has always spent his life working. Dangerous and degrading work has been the cause of death for many people for centuries. 
In this sense, there has always been a tendency to try to relieve man of the heaviest jobs by looking for machines or automatic systems to replace him.
In a sense, with the advent of the industrial revolution, we witnessed the first real process of robotizing in history.
On the other hand, with the evolution of discoveries in the medical field, the desire and curiosity arose in man to try to clone himself, artificially constructing his own like.
It is here that these two needs and tendencies come together in what we now call humanoid robots.
Indeed, humanoid robots are designed and built to replace humans in the most physical and repetitive tasks, in order to ensure greater well-being.

\newpage

\section{History of Robotics}
In recent years, the general public has become increasingly interested in robots and robotics research. New developments, e.g. robotic competitions, which "push beyond the boundaries of current technological
systems" (such as Defense Advanced Research Projects Agency (DARPA) in the
United States), especially in the area of robotics, have promised and delivered
fully integrated systems, \citet{robocomp}.\\
But the idea of creating intelligent, useful machines for humans has existed since the beginning of mankind.
In fact, ever since civilisation, one of the most unattainable desires and ambitions for mankind has been to create artifacts of his own image.\\
From a historical perspective, the first example that can be interpreted as such dates back to 3500 B.C., with the legend of the giant Talus, the slave forged by Hephaestus.
Continuing in time and reaching the Babylonians in 1400 B.C., we can observe the creation of the first automatic machine, the clepsydra water clock.
Continuing through the centuries, creations became more and more technologically advanced and jumping back to the 1500s, we encounter Leonardo da Vinci and his many inventions.
\begin{figure}[H]
    \centering
    \includegraphics{Chapter 3/davinchiknight.jpeg}
    \caption{Leonardo da Vinci’s mechanical knight: sketches on the right, rebuilt
and showing its inner workings on the left.}
    \label{fig:my_label}
\end{figure}
The concept of the robot then gradually entered people's minds thanks to this long process, but it was only in the 20th century that it took on a real physical connotation.
\newline
The term 'robot' was introduced in 1920 by the play 'Rossum's Universal Robot', by Karel Čapek: it derives etymologically from the Slavic root word 'robota' meaning subordinate labor.
\begin{figure}[H]
    \centering
    \includegraphics{Images/Chapter 3/rossumplay.jpg}
    \caption{A scene from Rossum's Universal Robot play, showing three robots}
    \label{fig:rossum}
\end{figure}
Later, during the middle of the century, the first research into the connection between human and machine intelligence was undertaken, marking the beginning of Artificial Intelligence (AI).
Between 1950 and 1980, Isaac Asimov wrote the so called "Three Laws of Robotics" in his book 'Runaround'. They are encoded in the "positronic brains" and are defined as follows, \citet{asimov}:
\begin{itemize}
\item A robot may not injure a human being or, through inaction, allow a human
being to come to harm.
\item A robot must obey the orders given to it by human beings, except where
such orders would conflict with the First Law.
\item A robot must protect its own existence as long as such protection does not
conflict with the First or Second Law.
\end{itemize}
Around those years, the first robots were created, they stemmed from the confluence of advances in two fields: numerically controlled machines for precision manufacturing and remote control to handle highly radioactive materials.
In fact, these two fields already featured modern applications of technologies such as mechanics, control, computational science and electronics.
The first robots were therefore master-slave arms, designed to reproduce the mechanics of the human arm but with rudimentary control and little perception.\\
During the second half of the century, the development of integrated circuits, digital computers and miniaturised components allowed terminal-controlled robots to be designed and developed.\\
In fact, in the 1980s, robotics was defined as the science that studies the connection between action and perception.In fact, action involves a locomotion apparatus that moves in the environment and a manipulation apparatus that performs actions, modifying its surroundings, thanks to special actuators and end-effectors.\\
Perception is then extracted from the sensors that provide information about the state of the robot (e.g. position and speed) and its surroundings (e.g. range and vision).
In the 1990s, research was further accelerated by the need to rely on robots to replace human presence in critical environments.\\
As we enter the new millennium, robots have undergone profound transformations both in their scope of use and in their shapes and sizes. 
\subsubsection{Humanoid Robots}
As reported in the article "Humanoid Robots: Historical Perspective, Overview and Scope", \citet{Siciliano2020}:\\
\\
"\textit{The long saga of humanoid robots in science fiction has influenced generations
of researchers, as well as the general public, and serves as evidence that people
are drawn to the idea of humanoid robots. Humans generally like to observe and
interact with one another. In their social behavior, people are highly attuned to
human characteristics, such as the sound of human voices and the appearance of
human faces and body motion. \\
Infants show preferences for these types of stimuli at
a young age, and adults appear to use specialized mental resources when interpreting these stimuli. By mimicking human characteristics, humanoid robots can engage
these same preferences and mental resources.\\
Throughout history, the human body and mind have inspired artists, engineers,
and scientists, using media as diverse as cave paintings, sculpture, mechanical toys,
photographs, and computer animation. \\
Humanoid robots serve as a powerful new
medium that enables the creation of artifacts that operate within the real world
and exhibit both human form and behavior. 
\\The field of humanoid robotics focuses
on the creation of robots that are directly inspired by human capabilities and/or
selectively imitate aspects of human form and behavior. Humanoids come in a
variety of shapes and sizes, from complete human-size legged robots to isolated
robotic heads with human-like sensing and expression.}"\\
Thus, humanoid robots were developed to be employed as multi purpose mechanical workers, and were designed to work alongside humans in daily tasks, being a support, living in the same environment and using the same tools.
It must also be considered that when the robot moves around in the work environment, there can be multiple risks for the worker; in this respect, a subfield of robotics, called cognitive robotics, has taken hold.
Indeed, robots can take advantage of the traditional communication methods used among humans to become more aware of their surroundings.
An even more ambitious aim is to interpret human gestures through vision (eye gaze, body language). On the other hand, this could put a human in a difficult relationship with the robot, modelled by the phenomenon called 'uncanny valley', a concept introduced in the 1970s by Masahiro Mori, a professor at the Tokyo Institute of Technology.
Masahiro in fact argues that:\\
"\textit{I have noticed that, in climbing toward the goal of making robots appear human, our affinity for them increases until we come to a valley, which I call the uncanny valley.}"
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{Images/Chapter 3/Mori_Uncanny_Valley.png}
    \caption{Mori Uncanny Valley}
    \label{fig:mori_uncanny_valley}
\end{figure}
Mori better explains this concept with the example of the prosthetic hand:
\\
"\textit{One might say that the prosthetic hand has achieved a degree of resemblance to the human form, perhaps on a par with false teeth. However, when we realize the hand, which at first site looked real, is in fact artificial, we experience an eerie sensation. For example, we could be startled during a handshake by its limp boneless grip together with its texture and coldness. When this happens, we lose our sense of affinity, and the hand becomes uncanny.}"
\\
On the other hand, many scientists and researchers in the robotics community see humanoid robots as a possibility to better investigate human nature itself.
A part from the roles mentioned above, a humanoid robot could work as an avatar for telepresence, test ergonomics and serve for any other  roles that a human can do.
Even though in the past decades, humanoids have only been applied in research field, times seem to be mature to put these robots on field and let them cooperate with humans.
\section{Robee: Oversonic Robotics configuration}
In order to make physical sense of the results obtained within this project, it is important to define what technologies were used and what materials made up Robee's hardware.
\subsection{Hardware components and software architecture}
It is important to bear in mind that the Oversonic project has an architecture split between the
robot (also referred to as the edge) and the cloud, and these two components coexist in a
hybrid.
Describing the system from the cloud, the hardware component consists of a scalable node pool based on
the 2.35Ghz AMD EPYC 7452 processor that can achieve a boosted maximum frequency
of 3.35GHz with 32 GB RAM memory, running a Kubernetes instance on top of Linux
Ubuntu 18.04 (Bionic Beaver).
As far as the robot is concerned, all the computational power is provided by 2 Intel NUCs 8 including an Intel Core i5-8259U Processor (6M Cache, up to 3.80 GHz), 8 GB RAM and Integrated Graphics Intel Iris Plus 655.
The operating system which is mounted on is Linux Ubuntu 20.04 (Focal Fossa), and all the modules are running containers that on turn are managed by KubeEdge, a containers orchestration system built on Kubernetes.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.28]{Images/Chapter 3/oversonicarch.png}
    \caption{Oversonic Architecture}
    \label{fig:oversonicarch}
\end{figure}
\textbf{Internet of Things}\\
In the case of the Robee project, the architecture is therefore composed of various software modules that are containerised and must be able to communicate with each other.
The MQTT protocol is an optimal choice for this case.\\
From the official MQTT.org site: "\textit{MQTT is an OASIS standard messaging protocol for
the Internet of Things (IoT). It is designed as an extremely lightweight publish/subscribe
messaging transport that is ideal for connecting remote devices with a small code footprint and minimal network bandwidth. MQTT today is used in a wide variety of industries, such as automotive, manufacturing, telecommunications, oil and gas}", \citet{mqtt}.\\
MQTT therefore operates at the application layer of the OSI model, relying on TCP at the transport layer.
The MQTT protocol establishes two kinds of entities in the network: a message broker and a number of clients. The broker is nothing more than a server that receives all messages from all clients and then routes these messages to the relevant destination clients. A client is anything that can interact with the broker to exchange messages. The messages are routed to clients basing
on topics: every message is published over a specific topic, and only the clients subscribed
to it will receive the message.\\ A client, therefore, can be an IoT sensor or an application in a data centre that processes IoT data.
Each MQTT message has a command and a payload. The command defines the type of message:
\begin{itemize}
    \item CONNECT: initial message sent from client to broker, to instantiate a new connection
    \item DISCONNECT: final message sent from client to broker to end the connection
    \item PUBLISH: command to publish a message over a specified topic, it is sent from client to broker and then routed from broker to every client that appears to be subscribed to that topic
    \item SUBSCRIBE: message sent from client to broker in order to request a subscription to a specified topic
\end{itemize}
All MQTT libraries provide simple ways to handle such messages directly and can automatically populate certain required fields, such as 'message' and 'client Id'.
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.8]{Images/Chapter 3/mqtt.png}
    \caption{The MQTT publish and subscribe model for IoT sensors}
    \label{fig:mqtt}
\end{figure}
\subsection{Robots Configurations}
The robots covered by the work in this thesis are mainly three 
\begin{itemize}
    \item R007 is a small autonomous mobile robot used by Oversonic as a prototype in the testing phase and features skid steering kinematics. In fact, it has two belts with two torque motors. The system is based on an Intel NUC and peripherals: two or four lidar sensors, a tracking camera and a depth camera mounted on the top base. The use of this AMR (autonomous mobile robot) is mainly conceived in conjunction with its larger 'brother' Robee or in industrial logistics environments.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.03]{Images/Chapter 3/r007.jpg}
        \caption{R007}
        \label{fig:r007}
    \end{figure}
    \newpage
    \item R012 is the humanoid robot developed in Oversonic Robotics, now in its fourth evolution from the initial prototype and featuring a differential drive base. The system is divided into two parts, a lower body and an upper body, each featuring an NUC terminal and several sensors, but in this analysis we will focus exclusively on the lower part. 
    The lower body in fact contains within it the two torque motors, which move two wheels actively. For the robot's stability, two passive caster wheels have been added to the front and rear of the base. Two Lidar sensors are then mounted on the base and going up about halfway up the torso are a tracking and depth camera. 
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.10]{Images/Chapter 3/r012.PNG}
        \caption{Robee R012}
        \label{fig:r012}
    \end{figure}
    \item N002 is a robot used for the testing phase of Robee's lower body. It has a similar purpose of use to R007.
    It has a differential drive base with two torque motor actuators, like R012 but with a physical arrangement of peripheral sensors but only one NUC.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.10]{Images/Chapter 3/n002.jpg}
        \caption{N002}
        \label{fig:N002}
    \end{figure}
\end{itemize}
The main features of the skid steering and differential drive kinematics will be listed.
\textbf{Skid Steering}

Skid Steering is a particular kinematics configuration featuring two tracks. It is composed of two tracks on its basic configuration, left and right, and the control variables are indeed left and right speed.
Another configuration entails a 4 wheels set up featuring a low wheelbase configurations so that they can be deemed as two tracks.
When rotating the central point does not move as the track is sliding on the ground. It is clear that this drive needs proper calibration and slippage modeling in order to be reliable.
Some assumptions are needed for this kinematics model: the first, mass is placed in center of the fictitious medium, the second, all the wheels on the same side have the same speed.
While in motion, this kind of drive presents multiple ICR and all of them share the same $\omega_{z}$.
\begin{equation}
\begin{bmatrix}
    v_{x}\\
    v_{y}\\
    \omega_{z}
\end{bmatrix}
= J_{\omega} \begin{bmatrix}
\omega_{l}r\\
\omega_{r}r
\end{bmatrix}\end{equation}

 The wheels are turning and sliding simultaneously, resulting in two fictitious instantaneous centers of rotation: $ICR_{left}$ and $ICR_{right}$.
 Under proper assumptions, skid-steering can be simplified to a differential drive kinematics.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.40]{Images/Chapter 3/skidsteer2.png}
    \caption{Skid Steering Kinematics}
    \label{fig:skidsteer}
\end{figure}
 Under proper assumptions, skid-steering can be simplified to a differential drive kinematics.
 \newpage
\textbf{Differential Drive}

Differential drive configuration present the following construction:
\begin{itemize}
    \item two wheels working on the same axis
    \item two independent motors, one for each wheel
    \item one or two passive caster wheels
\end{itemize}

Control input in this case are the linear and the angular velocity of the robot, \textit{v} and {\textomega}.
Wheels move around an Istantaneous Centre of Rotation on a circular path with istantaneous radius R and angular velocity \textomega.

\begin{equation}
    \begin{bmatrix}
    x' \\
    y' \\
    \theta \\
    \end{bmatrix} = \begin{bmatrix}
    \cos{\omega \delta t} & -\sin{\omega \delta t} & 0 \\
    \sin{\omega \delta t} & \cos{\omega \delta t} & 0 \\
    0 & 0 & 1
    \end{bmatrix}\begin{bmatrix}
    x - ICR_{x} \\
    y - ICR_{y} \\ 
    \theta'    \end{bmatrix} + \begin{bmatrix}
    ICR_{x} \\
    ICR_{y} \\
    \omega \delta t
    \end{bmatrix}
\end{equation}

It is therefore possible to reconstruct robot pose from direct kinematics:

\begin{equation}
    x(t) = \frac{1}{2}(\int_{0}^{t} (V_{R}(t') + V_{L}(t')) \cos{\theta(t')} \,dt')
\end{equation}

\begin{equation}
    y(t) = \frac{1}{2}(\int_{0}^{t} (V_{R}(t') + V_{L}(t')) \sin{\theta(t')} \,dt')
\end{equation}

\begin{equation}
    \theta = \frac{1}{b}(\int_{0}^{t} (V_{R}(t') - V_{L}(t')) \,dt')
\end{equation}


where $V_{R}$ and $V_{L}$ are defined as follows:
\begin{equation}
    V_{R} = \omega (R + \frac{b}{2}) 
\end{equation}

\begin{equation}
        V_{L} = \omega (R - \frac{b}{2})
\end{equation}

and as a consequence the following is derived:
\begin{equation}
    V = \frac{V_{R} + V_{L}}{2}
\end{equation}
\begin{equation}
    \omega = \frac{V_{R} - V_{L}}{2}
\end{equation}

It becomes clear that we can compute robot odometry by integrating the so-called control variables and knowing the parameters of the wheels, namely the direct kinematics.
On the contrary, we can derive control variables from a desired pose or velocity.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.10]{Images/Chapter 3/diffdrive.png}
    \caption{Differential Drive Kinematics}
    \label{fig:diffdrive}
\end{figure}

It is interesting to mention three borderline cases for this kind of drive:
\begin{itemize}
    \item $V_{L}$ = $V_{R}$: forward linear motion is straight
    \item $V_{L}$ = - $V_{R}$: rotation in place
    \item $V_{L}$ = 0 or $V_{R}$ = 0: respectively, rotation about left and right wheel
\end{itemize}
\subsection{Sensors}
In order for a robot to perceive the world around it and to complete tasks autonomously, sensors are required. We distinguish between proprioceptive (internal state of the robot) and exteroceptive (state of the external environment) sensors. In this section, we focus on the exteroceptive sensors that have been used in Robee, providing a brief overview of their functioning.

\textbf{D400 Intel Depth Camera}

Depth cameras are a type of sensor widely used in robotic applications. They normally consist of two parts: a traditional digital camera, which captures RGB data, and a projector, which captures depth data.The depth system can work in several ways, for example by projecting a grid of light structured in a non-visible spectrum into a scene and then analysing the distortion created in this pattern to determine the distance and/or shape of any object placed in front, \citet{JONASSON2021112691}.
In our application, the main reason for using the d455 camera in Robee's lower body is ????distance measurement?????.
Traditional digital cameras shoot out an image as a grid of pixels in two dimensions. Each pixel is then associated with three values ranging from 0 to 255, which define the red, green and blue components, so black, for example, is (0,0,0) and a pure bright red would be (255,0,0). . This type of representation is called an RGB image. Thus, each image is composed of three channels each storing the values pertaining to each colour component.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{Images/Chapter 3/rgb.jpeg}
    \caption{Channel decomposition}
    \label{fig:rgb}
\end{figure}
In the case of a depth camera, on the other hand, the pixels have different numerical values associated with them, where the number represents the distance of the corresponding pixel from the camera, thus the depth.
Thus, by unifying this representation we will have a colour map where cooler colours represent closer obstacles, and warmer colours represent more distant obstacles, in depth.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Images/Chapter 3/depthmap.png}
    \caption{Depth-map representation}
    \label{fig:depthmap}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{Images/Chapter 3/inteld455.jpeg}
    \caption{D455 Intel Tracking Camera }
    \label{fig:d455}
\end{figure}

Therefore, there are two types of three-dimensional image formats, the first being RGB-D and the second being pointcloud.
The first has already been introduced above, and we recall that for each pixel, identified by the coordinates (x,y), four properties (R,G,B,depth) are associated.
The substantial difference between the point cloud and RGB-D data is that in the pointcloud, the coordinates (x,y) represent the real world value instead of integer values. When viewing the two types of data, in fact, the former is presented in a sparse structure, while the latter is based on grid-aligned images. 
A practical application of point cloud will be provided in chapter \ref{ch:pcl}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{Images/Chapter 3/pointcloudsample.png}
    \caption{Point Cloud sample}
    \label{fig:pointcloudsample}
\end{figure}
Thus, the point cloud can be constructed from RGB-D format images. In fact, by knowing an RGB-D dataset and the camera's intrensic values through a process called camera calibration. 
Since pointclouds are sets of disordered vectors, it is common for researchers to change the structure of the pointcloud data into 3D voxel grids.
The voxel grid geometry is in fact a grid of values in three dimensions, organised in layers of rows and columns.
The reason for this conversion also comes from the fact that one often then has to deal with deep learning models that expect highly regular input data formats.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{Images/Chapter 3/voxelgrid.png}
    \caption{Voxel Grid example: A 3D Convolutional Neural Network for Real-Time Object Recognition}
    \label{fig:voxel}
\end{figure}


\textbf{T265 Intel Tracking Camera}

The T265 tracking camera is designed to integrate odometry data from the robot. It is in fact an independent and robust support for visual-inertia odometry and re-localisation.
A key strength of visual-inertial odometry is that the various sensors available complement each other. The images from the visual sensors are supplemented by data from an onboard inertial measurement unit (IMU), which includes a gyroscope and accelerometer. The aggregated data from these sensors is fed into simultaneous localization and mapping (SLAM) algorithms.
The tracking is done by comparing the information collected by the two fish-eye cameras, which collect images at 30 fps, \citet{inteltracking}.
% \begin{figure}[H]
%     \centering
%     \begin{subfigure}
%             \includegraphics[scale=0.5]{Images/Chapter 3/t265diagram.jpg}
%     \caption{Block diagram of Intel T265}
%     \label{fig:t265block}
%     \end{subfigure}%
%     ~
%     \begin{subfigure}
%         \includegraphics[scale=0.5]{Images/Chapter 3/t265.jpg}
%         \caption{T265 Intel Tracking Camera used in Robee}
%         \label{fig:t265}
%     \end{subfigure}
% \end{figure}
\begin{figure}[H]
\centering
\begin{tabular}{cc}
\subfloat[Block diagram of Intel T265]{\includegraphics[scale=0.6]{Images/Chapter 3/t265diagram.jpg}}&
\subfloat[T265 Intel Tracking Camera used in Robee]{\includegraphics[scale=0.5]{Images/Chapter 3/t265.jpg}}\\

\end{tabular}
\end{figure}
\newpage
\textbf{YDLidar}

LiDAR (Light Detection And Ranging) identifies technology that measures the distance to an object by illuminating it with laser light, while at the same time being able to return high-resolution three-dimensional information about the surrounding environment. A LiDAR typically uses several components: lasers, photodetectors and readout integrated circuits (ROICs) with time-of-flight (TOF) capability to measure distance by illuminating a target and analysing the reflected light.

\begin{figure}[H]
\centering
\begin{tabular}{cc}
\subfloat[Lidar architecture]{\includegraphics[scale=0.7]{Images/Chapter 3/lidar.jpg}}&
\subfloat[Ydlidar model G4 used in Robee]{\includegraphics[width = 3in]{Images/Chapter 3/ydlidar.jpg}}\\
\end{tabular}
\end{figure}
YDLIDAR G4 is a 360-degree two-dimensional rangefinder 
developed by YDLIDAR. Based on the principle of triangulation, it is equipped with related optics, electricity, and algorithm design to achieve high-frequency and high-precision distance measurement. The mechanical structure rotates 360 degrees to continuously output the angle information as well as the point cloud data of the scanning environment while ranging. YDlidar provides a built in bridge to ROS that ease the integration with the overall system.
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Item}                              & \textbf{Min} & \textbf{Typical} & \textbf{Max} & \textbf{Unit} \\ \hline
\textbf{Ranging Frequency}                 & /            & 9000             & /            & Hz            \\ \hline
\multirow{3}{*}{\textbf{Ranging distance}} & 0.12         & /                & 16           & m             \\ \cline{2-5} 
                                           & 0.26         & /                & 16           & m             \\ \cline{2-5} 
                                           & 0.28         & /                & 16           & m             \\ \hline
\textbf{Field of View}                     & /            & 0-360            & /            & Deg           \\ \hline
\textbf{Angle Resolution} &
  \begin{tabular}[c]{@{}c@{}}0.2 \\ at 5 Hz\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}0.28 \\ at 7 Hz\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}0.48 \\ at 12 Hz\end{tabular} &
  Deg \\ \hline
\end{tabular}
\caption{Specifications  for Ydlidar G4}
\label{tab:ydlidar}
\end{table}
\subsubsection{Visual Fiducial System}
During the use and development of Robee's navigation, so-called Apriltags were used, a particular visual fiducial system chosen for its robustness and integration with the simulated Gazebo environment.
Visual fiducials are nothing more than artificial landmarks, designed to be easily recognisable within the working environment and distinguishable from one another.
The methodology is similar to that of a common QR code but with significant applications and objectives.
In fact, unlike a QR code, where the user has to frame the tag with the camera and capture the high-resolution snapshot, these types of tags are designed to work with a small amount of information (even as little as 12 bits) but with performance and ease of use that is clearly superior to QR codes. In fact, these types of tags are designed to be automatically detectable and localisable even in low resolution conditions, even providing the relative position and orientation of the tag with respect to the camera.
In terms of size, the Apriltags used range from 50 to 100 pixels, including the payload, \citet{olson2011tags}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{Images/Chapter 3/apriltag.jpg}
    \caption{AprilTag distance and orientation measurement}
    \label{fig:apriltag}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Images/Chapter 3/apriltagsteps.png}
    \caption{How AprilTag detection is performed}
    \label{fig:apriltagsteps}
\end{figure}
Visual fiducial systems have been used in robotics to improve human/machine interaction, enabling the development of commands such as 'follow me'.
In the context of this work, tags were used for SLAM (Simultaneous Localisation and Mapping), as an independent support to the sensors mentioned in the previous section.
In fact, in both the simulated and real environment, tags were placed in strategic positions in order to reposition the robot to the correct pose by comparing the information coming from the sensors and that coming from the tags.
AprilTag provides a package for perfect ROS integration.
\begin{figure}[H]
    \centering
    \includegraphics{Images/Chapter 3/aprilros.png}
    \caption{ROS TF}
    \label{fig:aprilros}
\end{figure}

The apriltag ROS package then takes as input the topic of the rectified image and returns a list of recognised tags and their positions in 3 dimensions.
However, in order to work, one must specify in the configuration files (settings and tags) which tag families to search for.
A practical application will be provided in chapter \ref{ch:sim_test}

\subsubsection{Indoor Positioning System}

For the purpose of this thesis, another technology that has been used is the Indoor Positioning System.
This is in fact a new approach to the problem of localisation when remote GNSS satellites, which are commonly blocked indoors, are not available.
There is now a wide offer for this type of solution, even with different communication protocols at its base: from Wi-Fi signals to Bluetooth to ultrasound.
In Robee, the choice was made to use an IPS system provided by Marvelmind, which is based on ultrasonic and time-of-flight (ToF) measurements with trilateration, \citet{yorke2021beacon}, and which also provides for communication via ROS topics, thus providing integration with the robotics platform used.
The system proposed by Marvelmind consists of a series of static beacons (four were used in our case), placed on the walls of the production area of Oversonic Robotics.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{Images/Chapter 3/beacon.jpg}
    \caption{MarvelMind beacons kit}
    \label{fig:beacon}
\end{figure}

Each beacon sends and receives a stream of hypersonic signals continuously. There is also a modem, which is connected to the PC on which the supplied software is run, and a beacon called a 'hedgehog' which is placed on the vehicle to be located, in this case Robee. The hedgehog then receives the signals from the four beacons and sends them to the modem, which proceeds to triangulate.
The communication frequency is customisable and directly affects localisation accuracy, which in the basic configuration is claimed to be +/- 2 cm.
